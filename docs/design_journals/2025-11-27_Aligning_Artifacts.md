Two pieces must be aligned before the compute engine can be implemented: the way quantized parameters are exported from Python and the way input data is presented to the PL. Both need to be consistent with the integer contract defined by `quantize_model.py` and `int8.py`.
The first alignment is between the binary artifacts and the BRAM layout. Currently, `quantize_model.py` writes weights as raw `.bin` files in NumPy C–order with shape `(out_features, in_features)` and biases as 1-D `.bin` vectors. In VHDL, the engine will address weights linearly as `addr = base + out_idx * IN + in_idx`. To remove any ambiguity, the export script will be extended to generate `.mem` or `.coe` files in exactly this order, iterating `out_idx` in the outer loop and `in_idx` in the inner loop. Each line in the memory file will hold a single INT8 weight value, so that BRAM can be synthesized as an 8-bit wide ROM indexed directly by this linear address. Biases will follow the same principle: one INT32 value per line, with line index equal to `out_idx`. This guarantees that the engine’s addressing scheme and the content layout are tightly coupled and that no reordering is needed between Python and RTL.

The second alignment concerns the input path and how quantization interacts with image preprocessing. The training and quantization pipeline assumes normalized inputs, using the standard MNIST transform chain followed by activation quantization (`ToTensor`, `Normalize`, then `quantize_input_fc1_in`) . For the final SoC, however, the PS will send **raw pixels** in the range `[0, 255]` directly to the PL; this is precisely the kind of work that is worth offloading to the FPGA. To maintain numerical equivalence with the Python INT8 model while still receiving raw pixels, a dedicated front-end block will be added in the PL: a 256-entry ROM that implements the exact mapping from `uint8` pixel value to the INT8 activation expected at `fc1_in`.

This ROM will not approximate the behaviour; it will be built offline using the same formulas already present in Python. A small tool will be added to the artifacts pipeline: it will read `int8_spec.json`, extract the quantization parameters for `fc1_in` (scale and zero-point) and replicate the preprocessing steps applied in software: convert `p` in `[0,255]` to float, apply the same normalization and then the same quantization rule used by `quantize_input_fc1_in`. For each pixel value, the resulting INT8 code is written to a `.mem` or `.coe` file with 256 lines. In hardware, the raw pixel from AXI-Stream will be used directly as an address into this ROM, yielding the quantized INT8 activation fed into the MLP engine. As a consequence, the PL sees exactly the same INT8 tensor that `int8.py` would have produced for the same raw image.

Once these two pieces are in place—the weight and bias memories exported in engine-friendly order and the input LUT ensuring raw-pixel equivalence, the `mnist_mlp_engine` can be implemented against a stable, fully specified numeric contract. The engine will no longer need to be aware of floating-point normalization or of how weights were originally arranged in PyTorch; it will simply read INT8/INT32 values from ROMs in known address order, consume INT8 activations coming from the input LUT, and perform the INT8×INT8→INT32 MAC and requantization exactly as the Python reference does.

