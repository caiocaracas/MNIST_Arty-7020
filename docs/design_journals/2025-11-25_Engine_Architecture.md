The numeric contract: all activations and weights use 8-bit integers; accumulators use 32-bit integers; each layer applies fixed requantization (scale M, shift, zero-point) and hidden layers apply ReLU activation. This alignment ensures one-to-one equivalence with the Python INT8 reference model and simplifies RTL verification and timing closure.

The compute core is configured for 4 input activations per cycle and 8 output neurons per cycle, yielding 32 simultaneous MAC operations each cycle. On a Zynq-7020 this mapping consumes ~32 DSP48E1 units, which leaves ample headroom while keeping routing and bank complexity low. We could push to higher parallelism (16 outputs per cycle or more) but judged that increased DSP usage and complexity would impose unacceptable risk for the project timeline.

The engine processes one image at a time using a layer-by-layer pipeline: image buffer -> Layer 1 -> Layer 2 -> Layer 3 -> Layer 4 -> Output. The same datapath and MAC array are reused across all layers. This reuse reduces duplication, lowers verification burden, and results in a predictable latency model.

Latency estimates based on the 784-128-64-32-10 topology show Layer 1 (784→128) requiring ~3,200 cycles, Layer 2 ~288 cycles, Layer 3 ~80 cycles, Layer 4 ~24 cycles, plus ~20 cycles for the final output streaming. Total compute latency is ~3,612 cycles or roughly 36 us at a 100 MHz clock. In comparison, input transfer of 784 bytes (196 words of 32-bit) via AXI stream requires ~196 cycles (≈2 µs). The system is therefore clearly compute-bound rather than IO-bound.

Weight data resides in BRAM with simple linear addressing: base_address + out_index × IN_size + in_index. Activation buffers are small (128, 64, 32, 10 entries) and implemented in distributed or small BRAM blocks supporting four parallel reads per cycle. The memory organization is intentionally simple, prioritizing timing closure and verification stability over maximal density or ultra-aggressive packing.

Control is managed by a deterministic FSM: setup layer -> load bias group -> MAC loop group -> quantization/activation group -> next layer -> output -> done. No inter-image pipelining or dynamic compute reconfiguration is required. This decision preserves clarity, verification ease, and predictable resource usage.

Higher degrees of parallelism (P_OUT=32 or K_IN larger) were considered but rejected due to the trade-offs: significant DSP and BRAM overhead, increased routing difficulty, risk to timing closure, larger verification surface, and diminished returns for the learning scope of this accelerator project. INT8 is maintained throughout to preserve equivalence with the reference model; moving to INT16 would require extensive re-verification, DSP mode changes, and memory redesign, which is out of scope.

Streaming pixels directly into the MAC engine during DMA was also evaluated but ruled out: while theoretically attractive, it would require tight coupling between DMA timing and compute engine scheduling, complicating testbench design and increasing risk of synchronization hazards. A buffer-first then compute model is chosen instead for reliability.

Next steps: implement the S_MAC_Group stage in VHDL generically parameterised by K_IN=4, P_OUT=8; build a dedicated engine-level testbench comparing each layer output to Python model; integrate the engine with the top-level MNIST_accel module and existing AXI infrastructure; perform synthesis and implementation; measure DSP/LUT/BRAM usage, verify worst-case latency; prepare summary for final report.