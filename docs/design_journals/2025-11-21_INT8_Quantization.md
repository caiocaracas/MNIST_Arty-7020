The quantization procedure is fully defined by the Python tooling developed for this project. What was initially a conceptual description of an INT8 pipeline has now been anchored in a concrete sequence of scripts that establish the numeric contract used by both the software reference and the FPGA implementation. The end result is an integer model whose behaviour is completely reproducible: all scaling parameters, zero-points and multipliers are computed offline and written into `int8_spec.json`, which becomes the single source of truth for the hardware architecture.

The process begins with a FP32 model trained using `train_fp32.py`. The network parameters remain in floating point, but a calibration step is performed before quantization. Using `collect_activation_stats.py`, the model runs across a subset of the MNIST training set, after normalization, and records running minima and maxima for each activation tensor (`fc1_in`, `fc1_out`, `fc2_out`, `fc3_out`, `fc4_out`). Statistics are collected strictly after the ReLU boundaries defined by the FP32 model, meaning quantization parameters are learned against the actual distribution produced by the real forward path rather than synthetic ranges. These min/max pairs define the true dynamic interval that must be representable in signed INT8. Each activation tensor then receives its own scale and zero-point. The formula used in `quantize_model.py` computes scale as `(max - min) / (qmax - qmin)` with `qmin = -128` and `qmax = 127`. The zero-point is chosen so that real zero aligns to an element in the representable INT8 domain, clamped to remain within bounds. Because min and max typically straddle zero only slightly after ReLU, most output activations end up with a zero-point close to zero. Input to the first layer follows the same rule, but its statistics come directly from flattened normalized images rather than from a hidden layer.

Weights use symmetric per-tensor quantization. The procedure extracts the maximum absolute value from the FP32 weight matrix and computes a scale such that the largest magnitude maps to ±127. This produces an INT8 matrix where zero-point is structurally zero. Biases are quantized to INT32 using the mathematically correct scale product `S_in × S_w` which mirrors the accumulator’s implicit scaling in the hardware. This ensures that bias addition is performed in the same integer domain as the MAC sums, avoiding inconsistencies or hidden floating-point adjustments.

A key part of the quantization logic is the construction of the requantization parameters for each layer. After accumulation, the INT32 output must be mapped back to INT8. The real conversion factor `(S_in × S_w) / S_out` is approximated by searching over all shifts from 0 to 31 and selecting the integer M such that `M / 2^shift` minimizes the absolute error relative to the true multiplier. The algorithm in `quantize_model.py` performs an exhaustive discrete scan, producing a pair `(M, shift)` that fits cleanly into hardware as a single 32×32 multiply followed by a right-shift with rounding. This step is essential because the FPGA implementation will not perform floating-point division; all scaling must conform to this integer approximation.

Once quantized, the weight and bias tensors are written to raw `.bin` files, and a comprehensive specification is serialized to `int8_spec.json`. This document embeds the architecture description, all activation quantization parameters, all layer-wise requantization factors, and the mapping between tensor names and their roles inside the pipeline. The structure is built so that `int8.py` can reconstruct the entire network using strictly integer arithmetic without referencing any floating-point quantity during inference. The Python integer forward pass performs the same steps intended for the hardware: subtract input zero-points, perform INT8×INT8 MACs accumulated in INT32, add INT32 bias, apply M and shift, reapply output zero-point, clamp to INT8, and apply ReLU only when the layer semantics require it.

This process directly enforces the hardware contract. The accelerator in RTL is expected to operate under exactly the same numeric rules: the same sign conventions, the same shifting strategy, the same linear algebra, and the same representation of ReLU in the integer domain. All quantization decisions are made offline, fixed at synthesis time, and never inferred on hardware. That rigidity is intentional; it allows the VHDL implementation to remain simple and deterministic while guaranteeing bit-accurate agreement with the Python INT8 model.

The quantized model is not optimized for minimal error; it is shaped for structural coherence. The emphasis is on tractable scaling, reproducible rounding behaviour, and a clean mapping to DSP48E1 arithmetic patterns. The resulting INT8 inference path is thus not merely an approximation of the FP32 network but a stable numeric specification designed to be implemented exactly as-is inside the accelerator’s datapath.