The system is organized around a clear division of responsibility. The PS handles orchestration, DMA setup and register programming, while the PL assumes complete control over the integer arithmetic and memory access patterns required for inference. The design avoids dynamic dispatch, algorithmic reconfiguration and adaptive scaling. All quantities the hardware must rely on are computed offline and written to a stable specification file consumed by both the Python reference model and the RTL.

The PL architecture follows the structure implied by the quantized MLP. A shared MAC cluster executes all layers uniformly. The numeric contract ensures that each stage of the computation is a direct integer analogue of the floating-point model: subtract zero-points, perform INT8Ã—INT8 multiplies accumulated into INT32, add INT32 biases scaled consistently with the input and weight scales, apply integer requantization with a fixed (M, shift) pair, insert ReLU only where it belongs, and write back INT8 outputs. The RTL does not make choices; it merely echoes the decisions captured in the quantization artifacts.

Data movement retains equal clarity. The entire input image (784 bytes) is transmitted once per inference using AXI-Stream. The PL holds weights, biases and activation buffers entirely in BRAM or distributed RAM. No DDR access exists along the compute path, ensuring latency is invariant and easy to reason about. The PS only retrieves the final logits. This keeps the compute phase isolated and deterministic, matching the goal outlined in the initial hypothesis.

The planning effort therefore focuses not on microarchitectural novelty but on reliable execution. The compute fabric is kept intentionally moderate in width to avoid routing congestion and timing hazards. Banking structures remain shallow and explicit so that Vivado can map the design consistently. The FSM driving layer execution is predictable and contains no opportunistic shortcuts. The ambition is to create a piece of hardware that behaves like a stable numerical function: cleanly defined, repeatable, and easy to validate against a software reference.

This design plan brings the software and hardware sides into alignment. The training and quantization pipeline defines the numeric environment; the accelerator implements exactly that environment; the PS provides only the minimum coordination necessary for inference. The system therefore serves as a coherent demonstration of SoC integration rather than an exercise in maximizing neural-network throughput.